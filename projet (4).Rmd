---
title: "MRR : Projet - rendu 1"
author: "Lina EL HADDAJ & Ania POLIDORI"
date: ' XXXXXX'
output:
  pdf_document:
    fig_caption: yes
  word_document: default
  html_document: default
fontsize: 10pt
---

```{=html}
<style type="text/css">
  body .main-container{
   max-width: 1100px !important;
   width: 1100px !important;
  }
  body {
    max-width: 1100px !important;
    margin = auto;
    padding: 5em;
  }
  body, td{
    font-size: 2em;
  }
  code.r{
    font-size: 1em;
  }
  pre{
    font-size: 1em;
    color: #191970;
  }
</style>
```
[**Remarque : accorder les bons facteurs (on a des chiffres mais c'est pas des numeric, pas d'itération --\> les définir correctement)**]{.underline}

```{r setup, include=FALSE}
rm(list = ls())
set.seed(200)
library(corrplot)
library(ggplot2)
library(dplyr)
library(glmnet)
library(GGally)
library(ROCR)
```


# Introduction

Nous avons un jeu de données composé d'**étudiants qui suivent différents cursus universitaires**. 

Dans celui-ci, de nombreuses informations sur l'étudiant au moment de son arrivée dans son cursus ainsi que des sur ses performances académiques aux premier et second semestres sont connues. 

Grâce à toutes ces informations récoltées, nous allons tenter de répondre à la problématique suivante : 

**Comment utiliser les caractéristiques démographiques, académiques et socio-économiques des étudiants pour prédire avec précision les abandons d'étudiants et les réussites académiques dans un contexte éducatif donné, à un stade tôt de leur chemin académique ?**


# Etude préliminaire du jeu de données

```{r, include=FALSE}
data <- read.csv(file = "data.csv", sep = ';', header = TRUE)

# structure du dataset
dim(data)
head(data)
str(data)

# valeurs manquantes
# complete.cases(data) --> retourne TRUE pour chaque case non vide
sum(is.na(data))

# renommer variable avec nom pas clair car problème d'importation sur un des pc
# data <- data %>% rename(Marital.status = ï..Marital.status)

# remise au bon format des variables à facteurs
data <- data %>% mutate_at(vars(Marital.status,
                                Application.mode,
                                Application.order,
                                Course,
                                Daytime.evening.attendance.,
                                Previous.qualification, 
                                Nacionality,
                                Mother.s.qualification,
                                Father.s.qualification,
                                Mother.s.occupation,
                                Father.s.occupation,
                                Displaced,
                                Educational.special.needs,
                                Debtor,
                                Tuition.fees.up.to.date, 
                                Gender,
                                Scholarship.holder,
                                International, 
                                Target
                                ), as.factor) 

# changement des noms des facteurs du genre (pour que ce soit plus intuitif dans les analyses)
data$Gender <- factor(data$Gender, labels=c("F", "M"))

```


Notre jeu de données est composé de **37 variables** et de **4424 observations**, dont chacune qui représente un étudiant et chaque variable une information sur cet étudiant, **sans valeurs manquantes**. 

A l'importation du jeu de données, toutes les variables ont été importées sous format numérique. Mais la plupart d'entre elles sont des variables catégorielles, on leur a donc accordé les bons facteurs. 

## Choix de la variable cible 

La variable cible choisie pour notre analyse est dérivée de la variable `Target`, qui représente le résultat académique des étudiants à la fin de la durée normale du cursus. Cette variable est formulée comme une tâche de classification à trois catégories, distinguant entre les étudiants qui ont abandonné (`dropout`), ceux qui sont inscrits (`enrolled`), et ceux qui ont obtenu leur diplôme (`graduate`).

Afin de simplifier la tâche de prédiction, nous avons créé une variable binaire, `Ybin`, où les étudiants diplômés sont représentés par 1, et les étudiants inscrits ou ayant abandonné par 0. Cette approche nous permet de concentrer notre modèle sur la prédiction de la réussite académique à la fin de la durée normale du cursus, en distinguant de manière binaire entre les étudiants qui obtiennent leur diplôme et ceux qui n'atteignent pas cet objectif, simplifiant ainsi la complexité de la tâche de classification initiale à trois catégories.

```{r, include=FALSE}
# création de la nouvelle variable cible
data$Ybin <- factor(ifelse(data$Target == "Graduate", 1, 0))

# graphique des proportions de la variable cible
ggplot(data = data, aes(x = "", fill = Ybin))+
  geom_bar(position = "fill",width = 1, color="white")+
  coord_polar("y")+
  scale_y_continuous(labels = scales::percent)+
  labs(title = "Distribution des étudiants étant graduate ou non", x="",y="")+
  scale_fill_discrete(name="Graduate", 
                      labels = c("Non", "Oui"))+
  theme_minimal() +
  theme(axis.title = element_blank())
```



## Statistiques descriptives

**Corrélations positives significatives :**

- Entre les notes de qualification précédente et les notes d'admission ($0.58$)
- Entre les variables liées aux unités d'enseignement du premier semestre et celles du deuxième semestre, indiquant une **cohérence dans la performance académique d'un semestre à l'autre**

**Corrélations négatives significatives :**

- Entre l'âge à l'inscription et les notes du premier semestre ($-0.16$), suggérant que **les étudiants plus jeunes ont tendance à obtenir de meilleures notes au premier semestre**.
- Entre le taux de chômage et le produit intérieur brut ($-0.34$), indiquant une relation inverse entre ces deux variables économiques.

```{r}
numeric_data <- cbind(data$Ybin, data[, sapply(data, is.numeric)])
colnames(numeric_data)[1] <- "Ybin"

# Calculer la matrice de corrélation
correlation_matrix <- cor(numeric_data %>% select(-Ybin))

# Vérifier les noms de colonnes
colnames(correlation_matrix)

# Tracer la matrice de corrélation avec corrplot
corrplot(correlation_matrix,
         tl.cex = 0.7, # Ajuster la taille des étiquettes
         title = "Matrice de Corrélation", # Ajouter un titre
         tl.srt = 45, # Incliner les étiquettes à 45 degrés
         width = 30, height = 30) # Ajuster la taille du graphique
```


```{r}
summary(data)
```

```{r, echo=FALSE}
sequence=c(2:4, 5:7, 8:10, 11:13, 14:16, 17:19)
par(mfrow = c(3,6))
for(i in sequence){
  boxplot(numeric_data[ , i] ~ Ybin, data=numeric_data,
       main = colnames(numeric_data)[i], xlab="Graduate", ylab = "Valeurs", cex.main = 0.5)
}
```

On a pu déjà observer les variables quantitatives de notre jeu de données par rapport à notre variable cible, afin de voir s'il y avait potentiellement un lien entre ces variables et la graduation. 

On peut par exemple voir que les variables **Curricular.units.1st.sem.grade** et **Curricular.units.2nd.sem.grade** qui représentent respectivement la moyenne de l'étudiant (entre $0$ et $20$) aux premier et second semestres, et **Curricular.units.2nd.sem.approved** qui représente le nombre d'unités d'enseignement (UE) validées au premier semestre, semblent avoir des valeurs ainsi que des médianes **très différentes en fonction de si l'étudiant a eu son diplôme à la fin de son cursus ou non**.



```{r}
# proportions de la variable Gender
round(prop.table(table(data$Gender)) * 100, 2)
```

```{r}
# calcul des pourcentages de la variable Gender en fonction de Ybin
percentages_gender <- data %>%
  count(Gender, Ybin) %>%
  group_by(Gender) %>%
  mutate(percentage = n / sum(n))

# graphique de la relation entre Ybin et Gender
ggplot(data, aes(x = Gender, fill = Ybin)) +
  geom_bar(position = "fill") +
  geom_text(data = percentages_gender,
            aes(label = scales::percent(percentage),
                y = percentage),
            position = position_fill(vjust = 0.5),
            show.legend = FALSE) +
  labs(title = "Relation entre le genre et la réussite académique",
       x = "Genre",
       y = "Pourcentage d'étudiants")+
  scale_fill_discrete(name = "Diplômé", labels=c("Non", "Oui"))+
  scale_y_continuous(labels = scales::percent)+
  theme_minimal()
```

Ensuite, parmi toutes les variables catégorielles du jeu de données, nous avons également analysé visuellement s'il semble y avoir une relation entre celles-ci et notre variable cible. 
   
Nous avons par exemple le genre des étudiants, dont $64,83\%$ sont des femmes et $35,17\%$ sont des hommes. 
   
Nous pouvons observer que **le genre semble avoir un lien avec la réussite académique de l'étudiant**. En effet, $35,2\%$ des hommes ne sont pas diplômés contre $57,9\%$ des femmes. 



```{r}
# proportions de la variable Scholarship.holder
round(prop.table(table(data$Scholarship.holder)) * 100, 2)
```

```{r}
# calcul des pourcentages de la variable Scholarship.holder en fonction de sYbin
percentages_scholarship <- data %>%
  count(Scholarship.holder, Ybin) %>%
  group_by(Scholarship.holder) %>%
  mutate(percentage = n / sum(n))

# graphique de la relation entre Ybin et Scholarship.holder
ggplot(data, aes(x = Scholarship.holder, fill = Ybin)) +
  geom_bar(position = "fill") +
  geom_text(data = percentages_scholarship,
            aes(label = scales::percent(percentage),
                y = percentage),
            position = position_fill(vjust = 0.5),
            show.legend = FALSE) +
  labs(title = "Relation entre la réussite académique et si l'étudiant est boursier",
       x = "Boursier",
       y = "Pourcentage d'étudiants")+
  scale_fill_discrete(name = "Diplômé", labels=c("Non", "Oui"))+
  scale_y_continuous(labels = scales::percent)+
  scale_x_discrete(labels = c("Non", "Oui"))+
  theme_minimal()
```


Nous avons également pu observer un **lien entre le statut boursier des étudiants et leur réussite académique**. $75,16\%$ des étudiants ne sont pas boursiers tandis que $24,84\%$ le sont. 
    
Nous pouvons donc nous demander si le fait d'avoir une bourse est un facteur permettant de prédire si l'étudiant va ou non être diplômé : seulement $24\%$ des étudiants boursiers ne sont pas diplômés, tandis que $59\%$ de notre population d'étudiants non boursiers ne l'est pas. 





# Modélisation 

Après ces analyses sur nos variables, nous allons maintenant tenter de modéliser nos données afin d'expliquer notre variable cible. 

Pour ceci, nous allons effectuer un modèle logistique complet, c'est-à-dire prenant en compte toutes les variables du jeu de données. Cette modélisation complète nous permettra déjà de savoir si au moins une des variables est significative dans notre modèle, et donc s'il est pertinent de tenter de modéliser la variable cible avec au moins une de ces variables prédictives. 

```{r}
data <- data %>% select(-Target)
mod_log <- glm(Ybin ~ . , data, family = binomial)
summary(mod_log)
```

On peut observer que plusieurs des variables présentes dans ce modèle ont une p-value < $\alpha = 5\%$ , tels que `Curricular.units.2nd.sem..approved.` ou `Tuition.fees.up.to.date`. 


# Echantillonnage 

On créé un échantillon apprentissage (train) avec $70\%$ du jeu de données initial et un échantillon test avec $30\%$ du jeu de données initial. Pour cela, on split le data set de façon aléatoire. 

```{r}
# échantillon train
# sort(sample(nrow(data), floor(nrow(data) * 0.75)))

indices <- sample(1:nrow(data), size = 0.70 * nrow(data))
train_set <- data[indices,]
```


```{r}
#échantillon test
test_set <- data[-indices,]
```



```{r}
summary(train_set)
summary(test_set)
```



# Régression RIDGE 

## Modélisation 

Afin de pouvoir effectuer la régression logistique RIDGE sur notre jeu de données, on va devoir transformer toutes nos variables factorielles en variables binaires car la régression RIDGE ajoute une pénalité aux coefficients du modèle, et cette pénalité est définie en terme de valeurs numériques des coefficients. 

```{r}
# jeu x_train avec toutes les variables factorielles transformées en binaire, sans la variable cible
x_train <- model.matrix(Ybin ~ . -1 , data = train_set)
x_test <- model.matrix(Ybin ~ . -1 , data = test_set)

# régression logistique RIDGE
mod_ridge <- glmnet(x = x_train , y = train_set$Ybin, family = "binomial", alpha = 0)

# pour avoir les coefficients de la régression, pour chacun des 100 lambdas pris 
# coef.glmnet(mod_ridge)
```

$\lambda$ est l'hyperparamètre dans une régression RIDGE. C'est le paramètre qui sert à pénaliser la fonction de coût, et qui sert donc à éviter le sur-entraînement (*overfitting*)

Quand on effectue la régression RIDGE avec la commande `glmnet`, on obtient un objet contenant plusieurs informations. Nous n'avons pas spécifié de valeur de $\lamba$, cet objet va donc contenir 100 valeurs de $\lambda$ différentes. Pour chacun de ces $\lambda$, nous allons avoir les coefficients associés après la pénalisation. 

La régression RIDGE ne va pas vraiment servir à faire de la sélection de variables, mais plutôt à enlever la multi colinéarité des variables, en minimisant l'erreur de prédiction tout en homogéinisant les valeurs des paramètres. 

```{r}
# graphique des coefficients en fonction des log des lambdas 
plot(mod_ridge, xvar = "lambda")
```

**graphique à expliquer**


## Cross validation

On effectue la validation croisée avec la commande `cv.glmnet`. Cela va permettre de partitionner les données en plusieurs sous-ensembles grâce à la méthode des **k-folds**. Cette méthode divise les données en plis (*folds*) de taille égale. On utilisera les données d'entraînement pour cela. 

Pour chaque pli, la validation croisée va ajuster le modèle sur les *k-1* plis restants (*ensemble d'entraînement*) et évalué sur le pli retenu (*ensemble de validation*). Ce processus est répété *k* fois, chaque pli servant une fois comme ensemble de validation. 

La validation croisée va permettre de trouver le $\lambda$ optimal, cest-à-dire le $\lambda$ qui maximise les performances moyennes sur les ensembles de validation, c'est à dire sur tous les plis.

Avec cette commande, le $\lambda$ choisi est celui qui minimise la déviance binomiale. 
**déviance binomiale à expliquer**

```{r}
# cross validation
# 10 k folds par défaut 
cv_ridge <- cv.glmnet(x = x_train, y = train_set$Ybin, family = "binomial", alpha = 0)

# lambda optimal
best_lambda_ridge <- cv_ridge$lambda.min
best_lambda_ridge
```

```{r}
# graphique de l'erreur quadratique moyenne en fonction des log lambda
plot(cv_ridge)
abline(v = log(cv_ridge$lambda.min), col = "blue", lty = 2, lwd = 2)
```

Il s'agit d'un graphique représentant tous les $\lambda$ qui ont été testés avec notre cross validation, en affichant leur log, par rapport à la déviance binomiale. 


```{r}
# Prédiction des probabilités
pred_probs_ridge <- predict(mod_ridge, newx = x_test, s = best_lambda_ridge, type = "response")

# Transformer les probabilités en prédictions binaires (0 ou 1)
pred_classes_ridge <- ifelse(pred_probs_ridge > 0.5, 1, 0)

# Affichage des prédictions
result_ridge <- data.frame(Prédiction = pred_classes_ridge, Probabilités = pred_probs_ridge)
result_ridge
```






# Courbes ROC et AUC

On va ensuite calculer l'AUC. C'est une mesure de performance permettant de mesurer le pouvoir prédictif du modèle. Elle peut se situer entre 1 et 0.5 : plus l'AUC est proche de 1, plus la qualité de prédiction du modèle est bien. Pour 0.5, on aura une prédiction aléatoire, ça sera une performance médiocre. 

On va également représenter la courbe ROC Cette courbe relie les points avec comme abscisse le taux de faux positifs (FPR) = 1-Spécifité au seuil S et en ordonnée la sensibilité (TPR) au seuil S, pour une grille de plusieurs seuils. 

```{r}
# Créez un objet de prédictions
pred_ridge <- prediction(pred_probs_ridge, test_set$Ybin)

# Évaluez les performances avec la courbe ROC
roc_perf_ridge <- performance(pred_ridge, measure="tpr", x.measure="fpr")

# courbe ROC
plot(roc_perf_ridge, colorize = TRUE, main = "Courbe ROC - Ensemble de test", print.cutoffs.at = seq(0, 1,by = 0.1), text.adj = c(1.2, 1.2), lwd = 3)
```

```{r}
# AUC
auc_ridge <- performance(pred_ridge, "auc")@y.values[[1]]
cat("AUC sur l'ensemble de test :", auc_ridge, "\n")
```

Cette valeur d'AUC est une indication positive de la capacité discriminante de notre modèle.
Un AUC de 0.92 suggère que le modèle a une excellente capacité à distinguer entre les étudiants qui réussissent et ceux qui échouent.
En d'autres termes, il est capable de classer correctement la plupart des étudiants en fonction des facteurs inclus dans le modèle.

```{r}
# Création de la matrice de confusion
conf_matrix_ridge <- addmargins(table(Prédiction = pred_classes_ridge, Réalité = test_set$Ybin))

# Affichage de la matrice de confusion
conf_matrix_ridge
```

-   **Vrais Positifs (VP) :** 578

    Cela représente le nombre d'étudiants pour lesquels le modèle a correctement prédit la réussite.

-   **Vrais** **Négatifs (VN) :** 548

    Cela représente le nombre d'étudiants pour lesquels le modèle a correctement prédit l'échec.

-   **Faux Positifs (FP) :** 69

    Cela signifie que le modèle a prédit à tort la réussite pour ces étudiants.

-   **Faux Négatifs (FN) :** 133

    Cela signifie que le modèle a omis de prédire la réussite pour ces étudiants.

Le modèle a une bonne capacité à prédire à la fois les succès (VP élevés) et les échecs (VN élevés).
Les faux positifs (prédire à tort le succès) sont relativement faibles, ce qui suggère que le modèle n'est pas trop optimiste dans ses prédictions.
Les faux négatifs (manquer la prédiction de réussite) sont plus élevés, indiquant que le modèle pourrait ne pas être aussi sensible à détecter les réussites.

```{r}
# Création d'une fonction pour afficher plusieurs métriques d'évaluation
evaluate_model <- function(actual, predicted) {
  confusion_matrix <- table(Prédiction = predicted, Réalité = actual)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
  recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
  f1_score <- 2 * (precision * recall) / (precision + recall)
  
  cat("Précision :", precision, "\n")
  cat("Rappel :", recall, "\n")
  cat("F-mesure :", f1_score, "\n")
  cat("Exactitude (Accuracy) :", accuracy, "\n")
  
  return(confusion_matrix)
}

# Utilisation de la fonction
conf_matrix <- evaluate_model(test_set$Ybin, pred_classes_ridge)
```

-   **Précision (Precision) : 0.8933539**

    Elle mesure la proportion de vrais positifs parmi les instances prédites comme positives.
    Dans votre cas, environ 89,34 % des étudiants prédits comme réussissant (diplômés) le sont réellement.

-   **Rappel (Recall) : 0.8129395**

    Également appelé sensibilité, il mesure la proportion de vrais positifs parmi toutes les instances réellement positives.
    Environ 81,29 % des étudiants réellement diplômés ont été correctement identifiés par le modèle.

-   **F-mesure : 0.8512518**

    C'est une moyenne harmonique de la précision et du rappel.
    Elle donne une mesure équilibrée entre les deux.
    Une valeur de 0,85 indique une bonne harmonie entre la précision et le rappel dans le modèle.

-   **Exactitude (Accuracy) : 0.8478916**

    Elle mesure la proportion totale de prédictions correctes (vrais positifs + vrais négatifs).
    Environ 84,79 % de toutes les prédictions du modèle sont correctes.

# Régression LASSO

```{r}
# Ajuster le modèle Lasso
mod_lasso <- glmnet(x = x_train, y = train_set$Ybin, family = "binomial", alpha = 1)

# Tracer les coefficients en fonction du logarithme des valeurs lambda
plot(mod_lasso, xvar = "lambda")
```

```{r}
# Validation croisée pour trouver la valeur optimale de lambda
cv_lasso <- cv.glmnet(x = x_train, y = train_set$Ybin, family = "binomial", alpha = 1)

# Afficher la valeur optimale de lambda
best_lambda_lasso <- cv_lasso$lambda.min
cat("Lambda optimal :", best_lambda_lasso, "\n")
```

```{r}
# Tracer les métriques de performance validées croisées
plot(cv_lasso)
```


```{r}
# Afficher la valeur optimale de lambda
meilleur_lambda <- cv_lasso$lambda.min
cat("Lambda optimal :", meilleur_lambda, "\n")
```

```{r}
# Extraire les coefficients pour la valeur optimale de lambda
coefficients_lasso <- coef(cv_lasso, s = meilleur_lambda)

# Afficher les coefficients
print(coefficients_lasso)
```

# Régression Logistique

```{r}
# Ajuster le modèle de régression logistique
mod_logistic <- glm(Ybin ~ ., data = train_set, family = "binomial")

# Résumé du modèle
summary(mod_logistic)
```


```{r}
# Liste des noms de colonnes catégorielles
categorical_columns <- c("Marital.status", "Application.mode", "Application.order", "Course", "Daytime.evening.attendance.", "Previous.qualification", "Nacionality", "Mother.s.qualification", "Father.s.qualification", "Mother.s.occupation", "Father.s.occupation", "Displaced", "Educational.special.needs", "Debtor", "Tuition.fees.up.to.date", "Gender", "Scholarship.holder", "International", "Ybin")

# Aligner les niveaux pour toutes les variables catégorielles
for (col in categorical_columns) {
  test_set[[col]] <- factor(test_set[[col]], levels = levels(train_set[[col]]))
}

predicted_test_logistic <- predict(mod_logistic, type = "response", newdata = test_set)
# pose problème, j'ai pas réussi à corriger l'erreur
```


# Étude des plus proches voisins ou k-NN

```{r}
# Installation des packages si non installés
# install.packages("rpart")
# install.packages("rpart.plot")

# Chargement des bibliothèques
library(rpart)
library(rpart.plot)

# Supposons que votre dataframe s'appelle "data"
# Choisissez les variables pertinentes
selected_vars <- c("Previous.qualification")

# Créez un sous-ensemble avec les variables sélectionnées
subset_data <- data[, c(selected_vars, "Ybin")]

# Construisez l'arbre de classification
tree_model <- rpart(Ybin ~ ., data = subset_data, method = "class")

# Visualisez l'arbre avec des informations détaillées
rpart.plot(tree_model, extra = 101, under = TRUE, type = 2, fallen.leaves = TRUE)

```